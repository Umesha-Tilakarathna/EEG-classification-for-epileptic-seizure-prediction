{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[315  85]\n",
      " [ 94 306]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       400\n",
      "           1       0.78      0.77      0.77       400\n",
      "\n",
      "    accuracy                           0.78       800\n",
      "   macro avg       0.78      0.78      0.78       800\n",
      "weighted avg       0.78      0.78      0.78       800\n",
      "\n",
      "Accuracy: 0.78\n",
      "\n",
      "Sensitivity: 0.77\n",
      "Specificity: 0.79\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "csv_file_path = 'wavelet_metrics.csv'  # Path to your CSV file\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Step 2: Separate features and labels\n",
    "X = data.drop(['Data', 'label'], axis=1)  # Drop non-numeric and target columns\n",
    "y = data['label']  # Extract labels (0 or 1)\n",
    "\n",
    "# Step 3: Ensure equal class distribution in train and test sets\n",
    "class_0 = data[data['label'] == 0]\n",
    "class_1 = data[data['label'] == 1]\n",
    "\n",
    "# Step 4: Split each class into 80% train and 20% test\n",
    "train_0, test_0 = train_test_split(class_0, test_size=0.5, random_state=42)\n",
    "train_1, test_1 = train_test_split(class_1, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 5: Concatenate the training and testing data for both classes\n",
    "train_data = pd.concat([train_0, train_1])\n",
    "test_data = pd.concat([test_0, test_1])\n",
    "\n",
    "# Step 6: Extract features and labels from the training and testing sets\n",
    "X_train = train_data.drop(['Data', 'label'], axis=1)\n",
    "y_train = train_data['label']\n",
    "X_test = test_data.drop(['Data', 'label'], axis=1)\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Step 7: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 8: Apply LDA for dimensionality reduction\n",
    "lda = LDA(n_components=1)  # Binary classification, so 1 component is sufficient\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "# Step 9: Train an SVM classifier on the LDA-transformed data\n",
    "svm = SVC(kernel='rbf', random_state=42)  # Using linear kernel\n",
    "svm.fit(X_train_lda, y_train)\n",
    "\n",
    "# Step 10: Predict and evaluate the SVM classifier\n",
    "y_pred = svm.predict(X_test_lda)\n",
    "\n",
    "# Step 11: Calculate the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Step 12: Calculate Sensitivity and Specificity\n",
    "sensitivity = tp / (tp + fn)  # True Positive Rate\n",
    "specificity = tn / (tn + fp)  # True Negative Rate\n",
    "\n",
    "# Step 11: Print evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"\\nSensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 143 candidates, totalling 1430 fits\n",
      "Best Parameters: {'C': 100, 'gamma': 0.2}\n",
      "Confusion Matrix:\n",
      "[[125  35]\n",
      " [ 49 111]]\n",
      "\n",
      "Sensitivity: 0.69\n",
      "Specificity: 0.78\n",
      "Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, make_scorer\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "csv_file_path = 'wavelet_metrics.csv'  # Path to your CSV file\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Step 2: Separate features and labels\n",
    "X = data.drop(['Data', 'label'], axis=1)  # Drop non-numeric and target columns\n",
    "y = data['label']  # Extract labels (0 or 1)\n",
    "\n",
    "# Step 3: Ensure equal class distribution in train and test sets\n",
    "class_0 = data[data['label'] == 0]\n",
    "class_1 = data[data['label'] == 1]\n",
    "\n",
    "# Step 4: Split each class into 80% train and 20% test\n",
    "train_0, test_0 = train_test_split(class_0, test_size=0.5, random_state=42)\n",
    "train_1, test_1 = train_test_split(class_1, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 5: Concatenate the training and testing data for both classes\n",
    "train_data = pd.concat([train_0, train_1])\n",
    "test_data = pd.concat([test_0, test_1])\n",
    "\n",
    "# Step 6: Extract features and labels from the training and testing sets\n",
    "X_train = train_data.drop(['Data', 'label'], axis=1)\n",
    "y_train = train_data['label']\n",
    "X_test = test_data.drop(['Data', 'label'], axis=1)\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Step 7: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 8: Apply LDA for dimensionality reduction\n",
    "lda = LDA(n_components=1)  # Binary classification, so 1 component is sufficient\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "# Step 9: Define the SVM model and hyperparameters for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 2, 3, 5, 10, 20, 30, 40, 50, 100, 200],      # Regularization parameter\n",
    "    'gamma': [0.0001, 0.001, 0.01, 0.02, 0.05, 0.08, 0.1, 0.2, 0.3, 0.5, 1],  # RBF Kernel coefficient\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for balanced cross-validation splits\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the SVM with RBF kernel\n",
    "svm = SVC(kernel='rbf')\n",
    "\n",
    "# Perform GridSearchCV to find the optimal hyperparameters\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_lda, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Step 10: Train the final SVM model with the best parameters\n",
    "best_svm = grid_search.best_estimator_\n",
    "best_svm.fit(X_train_lda, y_train)\n",
    "\n",
    "# Step 11: Predict on the test set\n",
    "y_pred = best_svm.predict(X_test_lda)\n",
    "\n",
    "# Step 12: Calculate the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Step 13: Calculate Sensitivity and Specificity\n",
    "sensitivity = tp / (tp + fn)  # True Positive Rate\n",
    "specificity = tn / (tn + fp)  # True Negative Rate\n",
    "\n",
    "# Step 14: Print evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nSensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
